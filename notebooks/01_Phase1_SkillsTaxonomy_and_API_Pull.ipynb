{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aim / Purpose\n",
    "**Build a skills taxonomy (~200â€“300 skills with aliases) that can later be used to extract skills from job postings.**\n",
    "\n",
    "\n",
    "**Do a first manual API pull to test the data pipeline, validate the structure of the data, and save a small sample of job postings.**\n",
    "\n",
    "\n",
    "**Document observations and insights to guide Phase 2 (automation) and Phase 3 (skill extraction & EDA).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taxnomy Refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current taxonomy size: 200\n"
     ]
    }
   ],
   "source": [
    "taxonomy_path = Path(\"../data/processed/skills_taxonomy.json\")\n",
    "with open(taxonomy_path, \"r\") as f:\n",
    "    taxonomy = json.load(f)\n",
    "\n",
    "print(f\"Current taxonomy size: {len(taxonomy)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated taxonomy size: 200\n"
     ]
    }
   ],
   "source": [
    "aliases = {\n",
    "    \"python\": [\"py\", \"python3\"],\n",
    "    \"javascript\": [\"js\", \"nodejs\", \"ecmascript\"],\n",
    "    \"r\": [\"r language\"],\n",
    "    \"sql\": [\"structured query language\"]\n",
    "}\n",
    "\n",
    "# Merge aliases into taxonomy\n",
    "for skill, alias_list in aliases.items():\n",
    "    if skill in taxonomy:\n",
    "        taxonomy[skill][\"aliases\"] = alias_list\n",
    "    else:\n",
    "        taxonomy[skill] = {\"source\": \"manual\", \"aliases\": alias_list}\n",
    "\n",
    "\n",
    "# Ensure no duplicates in aliases\n",
    "for skill, meta in taxonomy.items():\n",
    "    if \"aliases\" in meta:\n",
    "        meta[\"aliases\"] = sorted(list(set(meta[\"aliases\"])))\n",
    "\n",
    "# Sort skills alphabetically\n",
    "taxonomy = dict(sorted(taxonomy.items()))\n",
    "\n",
    "with open(taxonomy_path, \"w\") as f:\n",
    "    json.dump(taxonomy, f, indent=2)\n",
    "\n",
    "print(f\"Updated taxonomy size: {len(taxonomy)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Manual API Pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Api parameters\n",
    "import os\n",
    "import requests\n",
    "from datetime import datetime\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # loads your .env file\n",
    "\n",
    "# Load API keys from .env \n",
    "ADZUNA_APP_ID = os.getenv(\"ADZUNA_APP_ID\")\n",
    "ADZUNA_APP_KEY = os.getenv(\"ADZUNA_APP_KEY\")\n",
    "\n",
    "# Adzuna endpoint\n",
    "BASE_URL = \"https://api.adzuna.com/v1/api/jobs/us/search/1\"  # US example, page 1\n",
    "\n",
    "# Query parameters\n",
    "params = {\n",
    "    \"app_id\": ADZUNA_APP_ID,\n",
    "    \"app_key\": ADZUNA_APP_KEY,\n",
    "    \"results_per_page\": 10,\n",
    "    \"what\": \"data scientist\",  # test role\n",
    "    \"content-type\": \"application/json\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 10 job postings\n"
     ]
    }
   ],
   "source": [
    "# Api Request \n",
    "response = requests.get(BASE_URL, params=params)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    jobs_data = response.json().get(\"results\", [])\n",
    "    print(f\"Fetched {len(jobs_data)} job postings\")\n",
    "else:\n",
    "    print(\"API request failed with status:\", response.status_code)\n",
    "    jobs_data = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jobs_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated 10 jobs out of 10 fetched\n"
     ]
    }
   ],
   "source": [
    "#Validate Jobs\n",
    "validated_jobs = []\n",
    "for job in jobs_data:\n",
    "    if all([\n",
    "        job.get(\"title\"),\n",
    "        job.get(\"description\") and len(job[\"description\"]) >= 50,\n",
    "        job.get(\"location\") and job[\"location\"].get(\"display_name\"),\n",
    "        \"salary_min\" in job or \"salary_max\" in job\n",
    "    ]):\n",
    "        validated_jobs.append(job)\n",
    "\n",
    "print(f\"Validated {len(validated_jobs)} jobs out of {len(jobs_data)} fetched\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved sample jobs to ..\\data\\raw\\2025-10-02\\jobs_sample.json\n"
     ]
    }
   ],
   "source": [
    "# Save Raw Data\n",
    "raw_dir = Path(f\"../data/raw/{datetime.today().strftime('%Y-%m-%d')}\")\n",
    "raw_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "raw_file = raw_dir / \"jobs_sample.json\"\n",
    "with open(raw_file, \"w\") as f:\n",
    "    json.dump(validated_jobs, f, indent=2)\n",
    "\n",
    "print(f\"Saved sample jobs to {raw_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total skills in taxonomy: 200\n",
      "Jobs fetched: 10\n",
      "Jobs validated: 10\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total skills in taxonomy: {len(taxonomy)}\")\n",
    "print(f\"Jobs fetched: {len(jobs_data)}\")\n",
    "print(f\"Jobs validated: {len(validated_jobs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rehtr\\anaconda3\\envs\\NewEnv1\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3269: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from collections import Counter\n",
    "survey = pd.read_csv(\"../data/raw/survey_results_public.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 200 StackOverflow skills: 188\n",
      "Final taxonomy size after cleaning and merge: 200\n"
     ]
    }
   ],
   "source": [
    "\n",
    "TOP_N = 200  # Number of top StackOverflow skills to include\n",
    "\n",
    "# Phase 0 seed skills (must be in taxonomy)\n",
    "seed_skills = [\n",
    "    \"python\", \"r\", \"sql\", \"pandas\", \"numpy\", \"scikit-learn\",\n",
    "    \"tensorflow\", \"keras\", \"pytorch\", \"spark\", \"hadoop\",\n",
    "    \"aws\", \"azure\", \"gcp\", \"streamlit\", \"flask\", \"fastapi\",\n",
    "    \"docker\", \"kubernetes\"\n",
    "]\n",
    "\n",
    "skill_cols = [\n",
    "    \"LanguageHaveWorkedWith\", \"LanguageWantToWorkWith\",\n",
    "    \"DatabaseHaveWorkedWith\", \"DatabaseWantToWorkWith\",\n",
    "    \"PlatformHaveWorkedWith\", \"PlatformWantToWorkWith\",\n",
    "    \"WebframeHaveWorkedWith\", \"WebframeWantToWorkWith\",\n",
    "    \"DevEnvsHaveWorkedWith\", \"DevEnvsWantToWorkWith\",\n",
    "    \"SOTagsHaveWorkedWith\", \"SOTagsWantToWorkWith\"\n",
    "]\n",
    "\n",
    "# Extract & clean skills\n",
    "all_skills_list = []\n",
    "for col in skill_cols:\n",
    "    if col in survey.columns:\n",
    "        skills = (\n",
    "            survey[col]\n",
    "            .dropna()\n",
    "            .str.split(\";\")\n",
    "            .explode()\n",
    "            .str.lower()\n",
    "            .str.strip()\n",
    "        )\n",
    "        all_skills_list.extend(skills)\n",
    "\n",
    "# Count frequency of each skill\n",
    "skill_counts = Counter(all_skills_list)\n",
    "\n",
    "# Keep top N most common skills\n",
    "top_skills = [skill for skill, _ in skill_counts.most_common(TOP_N)]\n",
    "print(f\"Top {TOP_N} StackOverflow skills: {len(top_skills)}\")\n",
    "\n",
    "# Load existing taxonomy\n",
    "with open(\"../data/processed/skills_taxonomy.json\", \"r\") as f:\n",
    "    taxonomy = json.load(f)\n",
    "\n",
    "# Merge top skills + ensure seed skills\n",
    "for skill in top_skills + seed_skills:  # seed_skills guaranteed included\n",
    "    if skill not in taxonomy:\n",
    "        source = \"stackoverflow\" if skill in top_skills else \"seed\"\n",
    "        taxonomy[skill] = {\"source\": source}\n",
    "\n",
    "# Save cleaned taxonomy\n",
    "with open(\"../data/processed/skills_taxonomy.json\", \"w\") as f:\n",
    "    json.dump(taxonomy, f, indent=2)\n",
    "print(f\"Final taxonomy size after cleaning and merge: {len(taxonomy)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "---\n",
    "\n",
    "\n",
    "## Skills Taxonomy\n",
    "\n",
    "- Number of skills: 200\n",
    "- Sources: O*NET + seed list + StackOverflow\n",
    "\n",
    "---\n",
    "\n",
    "## Manual API Pull\n",
    "\n",
    "- Date of pull: 2025-10-02\n",
    "- Role(s) fetched: \"data scientist\"\n",
    "- Number of jobs fetched: 5\n",
    "- Number of jobs validated: 5\n",
    "- Any missing fields or anomalies: None\n",
    "\n",
    "---\n",
    "\n",
    "## Insights\n",
    "\n",
    "- Any common skill mentions? ['python', 'r', 'aws', 'c', 'spark']\n",
    "- Are salary fields mostly missing/present? Mostly missing\n",
    "- Location parsing observations: ['Raleigh, Wake County', 'Lake Mary, Seminole County', 'The Gap, Chicago', 'West Slope, Washington County', 'Dahlgren, King George County']\n",
    "\n",
    "---\n",
    "Skills taxonomy built and saved: `skills_taxonomy.json`  \n",
    "Manual API pull successful: 10 jobs fetched and validated  \n",
    "Common skills extracted: ['analyze', 'aws', 'c', 'dig', 'git']  \n",
    "Unique locations observed: ['Raleigh, Wake County', 'Lake Mary, Seminole County', ...]  \n",
    "\n",
    "**Next Steps:**  \n",
    "1. Automate weekly job fetching via `fetch_jobs.py`  \n",
    "2. Add monitoring and logging for pipeline  \n",
    "3. Prepare mapping function for skill extraction using updated taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
